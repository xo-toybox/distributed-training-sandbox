{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch to Scale: Hands-on Distributed Training (from the ground up)\n",
    "\n",
    "> Teaching you *what* `torch.distributed` and other frameworks are doing so you can scale model training the **right way**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this course\n",
    "\n",
    "- Aimed at taking you from knowing how to train a model in PyTorch, `transformers`, `axolotl`, etc on a single GPU (or many*)\n",
    "- Into someone who can train models **efficiently** on a cluster of 2 to 200 GPUs\n",
    "- Understand how `DistributedDataParallelism`, `Pipeline Parallelism`, etc are all implemented\n",
    "- Understand how these then scale when you get into 2D, 3D+ parallelism strategies\n",
    "- Aimed at doing so in a *practical* manor. Less focused on the math, more focused on the **doing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who this is *not* for:\n",
    "\n",
    "- Experts in `torch.distributed` (you likely already know everything we're talking about)\n",
    "- Someone brand new to PyTorch/Deep Learning (I'd prefer if you had a few months of knowledge, and understand things like basic tensor operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Layout:\n",
    "\n",
    "- Week 1:\n",
    "  - Introduction\n",
    "  - `nbdistributed`\n",
    "  - Data Parallelism\n",
    "    - `accelerate`/data sharding (recording)\n",
    "    - Distributed Data Parallelism from scratch\n",
    "- Week 2\n",
    "  - Zero Redundancy Algorithm\n",
    "    - Core concept (Guest speaker recording, Sylvain Gugger)\n",
    "    - ZeRO 1, 2, and 3 from scratch\n",
    "    - PyTorch FSDP and Hybrid Sharding\n",
    "    - Introduction to TorchTitan (via code)\n",
    "- Week 3\n",
    "  - Pipeline Parallelism from scratch\n",
    "  - FP8 and low-precision training workshop\n",
    "  - Pipeline Parallelism with torchtitan\n",
    "  - How to train your small MoE\n",
    "- Week 4\n",
    "  - Tensor Parallelism from scratch\n",
    "  - Tensor Parallelism with TorchTitan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rules:\n",
    "\n",
    "* You can use the core of `torch.distributed` but **not** any of the major classes (so `torch` `FullyShardedDataParallelism`, etc)\n",
    "* After we finish a lesson on a subject, *then* you may use it out-of-the-box (so after today's lecture, you may use `DistributedDataParallelism`)\n",
    "* We are *not* building a framework, we *are* implementing the broad strokes from scratch to learn how they work"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
